---
title: "CS 422 HW4"
output: 
  html_notebook:
    toc: yes
    toc_float: yes
author: Matthew Venter
---


### Loading librarys
```{r}
library(dplyr)
library(plyr)
library(factoextra)
library(caret)
library(keras)
library(tensorflow)
library(tidyverse)
library(e1071)
library(coop)
```

### Part 2.1-prelim Creating data frame needed
```{r}
countries.df <- read.csv("countries.csv", sep=",", header=T, row.names = "Country")
```
### Part 2.1-a-i Print Summary
```{r}
summary(countries.df) 
```
### Part 2.1-a-ii Plot a boxplot of all of the attributes
```{r}
boxplot(countries.df,log = "y")
print("The two outliers are China and India and they represent a huge population outlier compared to the rest of the countries")
```

### Part 2.1-b PCA Transformation
```{r}
options(digits = 3)
countries.pca <- prcomp(countries.df, scale = T)
```

### Part 2.1-c-i Summary of PCA Transformation
```{r}
summary(countries.pca)
print("4 components explain at least 90% of the variance")
```
### Part 2.1-c-ii Screenplot of PCA Transformation
```{r}
fviz_screeplot(countries.pca, choice = "variance")
```
### Part 2.1-c-iii Screenplot of PCA Transformation
```{r}
print("It looks like 3 dimesnsions should cover just over 80% (2 also looks like it would wokr)")
```
### Part 2.1-d  Print the PCA components
```{r}
countries.pca$rotation
```
### Part 2.1-d-i PC1 Analisis 
```{r}
print("PC1 is positivly correlated with GDP, Lifeexp, Oilcons, Tel")
print("PC1 is negativly correlated with HIV, Mil, Pop, Unempl")
print("countries with positive PC1 are generaly going to more developed than countries with negative correlation")
print("maybe the better off the people in the country are")
```
### Part 2.1-d-ii PC2 Analisis 
```{r}
print("PC2 is positivly correlated with GDP, Lifeexp, Mil, Oilcons, Pop, Tel")
print("PC2 is negativly correlated with HIV and Unempl")
print("countries with positive PC2 are generaly going be healthier and has more jobs than those that arent")
print("maybe the better off the country is (how developed)")
```

### Part 2.1-e Draw Biplot 
```{r}
fviz_pca_biplot(countries.pca, repel = TRUE, col.var = "#2E9FDF", col.ind = "#696969")
```
### Part 2.1-e-i Examine the rotated variables
```{r}
countries.pca$x[c(1,14,9),1:2]
```
### Part 2.1-e-ii PCA Analisis
```{r}
print("from what i explained from d-ii and d-i these values make complete sense")
```

### Part 2.2 Set-Up
```{r}
df <- read.csv("activity-small.csv")
set.seed(1122)
df <- df[sample(nrow(df)), ]
indx <- sample(1:nrow(df), 0.20*nrow(df))
test.df  <- df[indx, ]
train.df <- df[-indx, ]
label.test <- test.df$label
test.df$label <- NULL
test.df <- as.data.frame(scale(test.df))
test.df$label <- label.test
rm(label.test)
label.train <- train.df$label
train.df$label <- NULL
train.df <- as.data.frame(scale(train.df))
train.df$label <- label.train
rm(label.train)
rm(indx)
```
### Part 2.2-a Create Shallow Nueral Network
```{r}
X_train <- select(train.df, -label)
y_train <- train.df$label
y_train.ohe <- to_categorical(y_train)
X_test <- select(test.df, -label)
y_test <- test.df$label
y_test.ohe <- to_categorical(test.df$label)

model <- keras_model_sequential() %>%
  layer_dense(units = 10, activation="relu", input_shape=c(3)) %>%
  layer_dense(units = 4, activation="softmax")

model %>% 
  compile(loss = "categorical_crossentropy", optimizer="adam", 
          metrics=c("accuracy"))

begin <- Sys.time()
model %>% fit(
  data.matrix(X_train), 
  y_train.ohe,
  epochs=100,
  batch_size=1,
  validation_split=0.20
)
end <- Sys.time()

model %>% evaluate(as.matrix(X_test), y_test.ohe)

pred.class  <- model %>% predict_classes(as.matrix(X_test))
pred.prob   <- model %>% predict(as.matrix(X_test)) %>% round(3)

confusionMatrix(as.factor(y_test), as.factor(pred.class))
```
### Part 2.2-a-i Nural Network Accuarcy
```{r}
print("Overall accuracy was 0.805")
```
### Part 2.2-a-ii Nural Network Accuarcy Class Specifics
```{r}
print("Class 0: had a sensitivity of 0.902, specificity 0.986, and a balanced accuracy of 0.944")
print("Class 1: had a sensitivity of 0.714, specificity 0.942, and a balanced accuracy of 0.828")
print("Class 2: had a sensitivity of 0.771, specificity 0.967, and a balanced accuracy of 0.869")
print("Class 3: had a sensitivity of 0.857, specificity 0.860, and a balanced accuracy of 0.759")
```
### Part 2.2-b Testing for Time
```{r}
#begin <- Sys.time()
#model %>% fit(
#  data.matrix(X_train), 
#  y_train.ohe,
#  epochs=100,
#  batch_size=256,
#  validation_split=0.20
#)
#end <- Sys.time()
time.table.df <- read.csv("batchsize-time.csv", sep=",", header=T)
time.table.df
```
### Part 2.2-c Testing for Accuracy
```{r}
#I'm just using code from a and entering it into a data table
batch.acc.table.df <- read.csv("batch-acc.csv", sep=",", header=T)
batch.acc.table.df
```
### Part 2.2-d Create Shallow Nueral Network (with another hidden layer(s))
```{r}

model <- keras_model_sequential() %>%
  layer_dense(units = 20, activation="relu", input_shape=c(3)) %>%
  layer_dense(units = 100, activation="relu") %>%
  layer_dense(units = 20, activation="relu") %>%
  layer_dense(units = 4, activation="softmax")


model %>% 
  compile(loss = "categorical_crossentropy", optimizer="adam", 
          metrics=c("accuracy"))

model %>% fit(
  data.matrix(X_train), 
  y_train.ohe,
  epochs=100,
  batch_size=1,
  validation_split=0.20
)

model %>% evaluate(as.matrix(X_test), y_test.ohe)

pred.class  <- model %>% predict_classes(as.matrix(X_test))
pred.prob   <- model %>% predict(as.matrix(X_test)) %>% round(3)

confusionMatrix(as.factor(y_test), as.factor(pred.class))
```
### Part 2.2-d-i Nural Network Accuarcy
```{r}
print("Overall accuracy was 0.855")
```
### Part 2.2-d-ii Nural Network Accuarcy Class Specifics
```{r}
print("Class 0: had a sensitivity of 0.947, specificity 0.979, and a balanced accuracy of 0.963")
print("Class 1: had a sensitivity of 0.851, specificity 0.915, and a balanced accuracy of 0.883")
print("Class 2: had a sensitivity of 0.816, specificity 0.987, and a balanced accuracy of 0.902")
print("Class 3: had a sensitivity of 0.787, specificity 0.928, and a balanced accuracy of 0.858")
print("The performance of the NN was more accurate after my ajustments")
```
### Part 2.3 Creating Recomendation system
```{r}
ratings.df <- read.csv("ratings.csv", header =T)
movies.df <- read.csv("movies.csv", header =T, row.names = "movieId")
id <-20393813 %% 671
user.110.profile <- c(0.25,0.12,0,0.04,0.37,0.27,0.02,0.59,0,0,0.04,0,0,0.08,0.16,0.10,0.29,0.12,0,0)
movie.1.profile <- c(1,0,0,0,1,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0)
movie.2.profile <- c(1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0)
movie.3.profile <- c(1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0)
movie.4.profile <- c(0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
movie.5.profile <- c(1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0)
movie.6.profile <- c(1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0)
movie.7.profile <- c(0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0)
movie.8.profile <- c(0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
movie.9.profile <- c(0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0)
movie.10.profile <- c(0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0)
ten.movies.df <- read.csv("ten-movies.csv", header = T)
ten.movies.df$Similarity<- c(cosine(user.110.profile,movie.1.profile),
cosine(user.110.profile,movie.2.profile),
cosine(user.110.profile,movie.3.profile),
cosine(user.110.profile,movie.4.profile),
cosine(user.110.profile,movie.5.profile),
cosine(user.110.profile,movie.6.profile),
cosine(user.110.profile,movie.7.profile),
cosine(user.110.profile,movie.8.profile),
cosine(user.110.profile,movie.9.profile),
cosine(user.110.profile,movie.10.profile))
print("User 110 chose the following 10 movies: 79702,7153,99114,318,2028,107406,54259,54503,159061,2572")
print("Of these, the following 5 movies are recommended: ")
head(ten.movies.df[order(ten.movies.df$Similarity, decreasing= T),], n = 5)
```
